#Importing modules

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
%matplotlib inline
warnings.filterwarnings('ignore')

#Load the dataset

df = pd.read_csv('kaggle_dataset_forCustomer_segmentation')
df.head()

#Statisical info 
df.describe()

#datatype info 
df.info()


#Exploratory Data Analysis
sns.countplot(df['Gender'])   // categorical data visualization
sns.distplot(df['Age'])           // distribution plot in histogram
sns.distplot(df['Annual income'])
sns.distplot(df['Spending score'])

#Correlation Matrix
corr = df.corr()
sns.heatmap(corr, annot =True,cmap = 'coolwarm')

#Clustering without PCA 

i) Cluster on two features
df1 = df[['Annual income', 'spending score']]
df1.head()

sns.scatterplot(df1['Annual income'],df1['spending score'])

#KMeansclustering  - training

from sklearn.cluster import KMeans
errors = []
for i in range(1,11):
    kmeans = KMeans(n_clusters= i)   // no of cluster for model
    k.means.fit(df1)
    errors.append(kmeans.inertia_)  --> Kmeans inertia is just loss function

#Plot the results for elbow method
plt.figure(figsize = (13,6))
plt.plot(range(1,11),errors)
plt.plot(range(1,11),linewidth = 3, color = 'red', markers='8')
plt.xlabel('no of clusters')
plt.ylabel("Wcss')             ---> loss function output
plt.xticks(np.arange(1,11,1))
plt.show()


//choose the elbow cluster which would be the best one for Kmeans algorithm. Here = 5; also by scatter plot , we can see easily 5 cluster is more appropriate doing this.

#Fitting
km = KMeans(n_clusters = 5)
km.fit(df1)
y = km.predict(df1)
df1['label'] = y
df1.head()

sns.scatterplot(x= 'Annual Income', y = 'spending score', data = df1, hue = 'label',s=50,palette = ['red', 'green,'purple','blue','orange'])

ii)Cluster on three features

df2 = df[['Annual income', 'spending score','Age']]
df2.head()


#KMeansclustering  - training

from sklearn.cluster import KMeans
errors = []
for i in range(1,11):
    kmeans = KMeans(n_clusters= i)   // no of cluster for model
    kmeans.fit(df2)
    errors.append(kmeans.inertia_)  --> Kmeans inertia is just loss function

#Plot the results for elbow method
plt.figure(figsize = (13,6))
plt.plot(range(1,11),errors)
plt.plot(range(1,11),linewidth = 3, color = 'red', markers='8')
plt.xlabel('no of clusters')
plt.ylabel("Wcss')             ---> loss function output
plt.xticks(np.arange(1,11,1))
plt.show()


//choose the elbow cluster which would be the best one for Kmeans algorithm. Here = 5; also by scatter plot , we can see easily 5 cluster is more appropriate doing this.

#Fitting
km = KMeans(n_clusters = 5)
km.fit(df2)
y = km.predict(df2)
df2['label'] = y
df2.head()

#plotting for 3d
fig = plt.figure(figsize = (20,20))
ax = fig.add_subplot(111,projection = '3d')

ax.scatter(df2['Age'] [df2['label'] == 0],df2['Annual Income'] [df2['label'] == 0],df2['spending score'] [df2['label'] == 0],c = 'red', s = 50)

ax.scatter(df2['Age'] [df2['label'] == 1],df2['Annual Income'] [df2['label'] == 1],df2['spending score'] [df2['label'] == 1],c = 'blue', s = 50)

ax.scatter(df2['Age'] [df2['label'] == 2],df2['Annual Income'] [df2['label'] == 2],df2['spending score'] [df2['label'] == 2],c = 'green', s = 50)

ax.scatter(df2['Age'] [df2['label'] == 3],df2['Annual Income'] [df2['label'] == 3],df2['spending score'] [df2['label'] == 3],c = 'purple', s = 50)

ax.scatter(df2['Age'] [df2['label'] == 4],df2['Annual Income'] [df2['label'] == 4],df2['spending score'] [df2['label'] == 4],c = 'orange', s = 50)

ax.view_init(30,190)
ax.set_xlabel('Age')
ax.set_ylabel('Annual Income')
ax.set_zlabel('Spending score')
plt.show()
